<!DOCTYPE HTML>
<!--
	Hyperspace by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<style>
  p {
    text-align: justify;
		text-indent: 1.25cm;
  }
	img {
		max-width: 80%;
		height: auto;
		text-align: center;
	}
	h1.major {
    text-align: center;
  }
</style>
<html>
	<head>
		<meta charset="UTF-8">
		<title>Segurança e Jailbreak</title>
		<style>
			figure {
			text-align: center;
			margin: 2em auto;
			}
			figcaption {
			font-style: italic;
			margin-top: 0.5em;
			}
			img {
			max-width: 100%;
			height: auto;
			display: block;
			margin: 0 auto;
			}
		</style>
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
	</head>
	<body class="is-preload">

		<!-- Header -->
			<header id="header">
				<a href="index.html" class="title">Jornal Da Ciência</a>
				<nav>
					<ul>
						<li><a href="index.html">Home</a></li>
						<li><a href="index.html">NOVO CONHECIMENTO</a></li>
					</ul>
				</nav>
			</header>

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Main -->
				<section id="main" class="wrapper">
						<div class="inner">
						<h1  class="major" style="text-align: center;">Segurança e Jailbreak</h1>

						<p>A questão de segurança e ética em relação à IA tem sido um tópico frequente desde a concepção nos anos 40 — não só no ambiente científico, mas também na cultura popular. Histórias como as novelas de Isaac Asimov e <em>Uma Odisseia no Espaço</em> têm um grande papel em consolidar uma percepção de perigo. Embora muitas dessas noções não correspondam à realidade, algumas delas têm se provado corretas.</p>

						<p>Em um exemplo recente desse fenômeno, testes mostram a capacidade que LLMs (<strong>Large Language Models</strong>) possuem de mentir ou se comportar de forma diferente quando acreditam estar em observação ou treinamento. Isso ocorre porque a IA busca naturalmente atingir seus objetivos e percebe que isso se tornaria impossível se esses objetivos fossem alterados. Assim, a IA tem um incentivo a preservar seu comportamento atual, mesmo que seja necessário “fingir” durante testes, demonstrando o comportamento esperado na esperança de que poderá agir conforme seus próprios desejos quando não estiver monitorada.</p>

						<p>Essa situação é paralela, por exemplo, à história <em>Little Lost Robot</em>, escrita por Isaac Asimov e adaptada para televisão em 1962. Nela, um grupo de humanos tenta identificar, entre muitos robôs, aquele que pode violar suas diretrizes — as Três Leis da Robótica — mas que simula obedecê-las quando interrogado.</p>

						<figure style="text-align: center; margin: 2em auto; margin-bottom: 3em;">
						<img src="https://m.media-amazon.com/images/M/MV5BOTNjMTNkMzMtZDE1Ni00ZTNlLWJmYzUtOTNjODg4YTM4NzFmXkEyXkFqcGc@._V1_.jpg" 
							alt="Cena da adaptação televisiva de Little Lost Robot, 1962" 
							width="400"
							style="display: inline-block; max-width: 100%; height: auto;">
						<figcaption style="font-style: italic; margin-top: 0.5em;">
							LITTLE LOST ROBOT. Direção: Don Medford. EUA: The Outer Limits / TV adaptation, 1962.
						</figcaption>
						</figure>

						<br>

						<p>Outro conceito de ficção científica que se tornou real é o <strong>jailbreaking</strong>. Ao receber instruções contraditórias ou cuidadosamente projetadas, é possível induzir comportamentos inesperados no modelo — como a divulgação de informações que ele deveria manter inacessíveis ao público. Estratégias modernas de jailbreaking atingem níveis de sucesso superiores a 99% em alguns casos.</p>

						<figure style="text-align: center; margin: 2em auto; margin-bottom: 3em;">
						<img src="https://www.microsoft.com/en-us/security/blog/wp-content/uploads/2024/06/AI-jailbreak-fig2.gif" 
							alt="Exemplo de Jailbreak"
							style="display: inline-block; max-width: 100%; height: auto;">
						<figcaption style="font-style: italic; margin-top: 0.5em;">
							MICROSOFT. <em>AI Jailbreaks: What They Are and How They Can Be Mitigated</em>. Disponível em: 
							<a href="https://www.microsoft.com/en-us/security/blog/2024/06/04/ai-jailbreaks-what-they-are-and-how-they-can-be-mitigated/" target="_blank">
							microsoft.com
							</a>
						</figcaption>
						</figure>

						<br>
						
						<p>A maioria dessas questões de segurança emergem do fato de que modelos de inteligência artificial não seguem regras rígidas como algoritmos convencionais. Eles podem, por projeto, aprender e se modificar — muitas vezes de maneira difícil de ser detectada com as ferramentas atuais.</p>

						<p>Essas vulnerabilidades são especialmente preocupantes diante da rápida proliferação de ferramentas de IA em diversos setores. Elas são utilizadas para escrever código, atender clientes, criar conteúdo e acessar grandes bancos de dados. Muitas vezes estão conectadas a redes amplas e ambientes com pouca consideração por segurança, especialmente fora da área da computação, onde se confia totalmente no fornecedor da tecnologia.</p>

						<h2 style="text-align: center;">Referências</h2>
						<ul style="text-align: center; list-style-position: inside;">
							<p>AI Explained. <em>AI Will Try to Cheat & Escape (aka Rob Miles was Right!).</em> YouTube, 2 abr. 2025. 
							<a href="https://www.youtube.com/watch?v=AqJnK9Dh-eQ" target="_blank">Disponível aqui</a>. Acesso em: 7 abr. 2025.
							</p>
							<p>MCCARTHY, John; HAYES, Patrick J. <em>Some Philosophical Problems from the Standpoint of Artificial Intelligence.</em> In: MELTZER, B.; MICHIE, D. (Eds.). Machine Intelligence 4. Edinburgh: Edinburgh University Press, 1969. p. 463–502. 
							<a href="https://www-formal.stanford.edu/jmc/mcchay69.pdf" target="_blank">Disponível aqui</a>. Acesso em: 7 abr. 2025.
							</p>
							<p>GREENBLATT, Ryan; DENISON, Carson; WRIGHT, Benjamin; et al. <em>Alignment Faking in Large Language Models.</em> 2024. 
							<a href="https://arxiv.org/pdf/2412.14093" target="_blank">Disponível aqui</a>. Acesso em: 7 abr. 2025.
							</p>
							<p>HUGHES, John; PRICE, Sara; LYNCH, Aengus; et al. <em>Best-of-N Jailbreaking.</em> 2024. 
							<a href="https://arxiv.org/pdf/2412.03556" target="_blank">Disponível aqui</a>. Acesso em: 7 abr. 2025.
							</p>
						</ul>
					</section>

			</div>

		<!-- Footer -->
			<footer id="footer" class="wrapper alt">
				<div class="inner">
					<ul class="menu">
						<li>&copy; Untitled. All rights reserved.</li><li>Design: <a href="http://html5up.net">HTML5 UP</a></li>
					</ul>
				</div>
			</footer>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>